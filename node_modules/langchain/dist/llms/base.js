"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLM = exports.BaseLLM = void 0;
const gpt_3_encoder_1 = require("gpt-3-encoder");
const p_queue_1 = __importDefault(require("p-queue"));
const index_1 = require("./index");
const cache_1 = require("../cache");
const getCallbackManager = () => ({
    handleStart: (..._args) => {
        // console.log(args);
    },
    handleEnd: (..._args) => {
        // console.log(args);
    },
    handleError: (..._args) => {
        // console.log(args);
    },
});
const getVerbosity = () => true;
const cache = new cache_1.InMemoryCache();
/**
 * LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.
 */
class BaseLLM {
    constructor(callbackManager, verbose, concurrency, cache) {
        /**
         * The name of the LLM class
         */
        Object.defineProperty(this, "name", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "cache", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "callbackManager", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * Maximum number of concurrent calls to this chain,
         * additional calls are queued up. Defaults to Infinity.
         */
        Object.defineProperty(this, "concurrency", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "queue", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * Whether to print out response text.
         */
        Object.defineProperty(this, "verbose", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        this.callbackManager = callbackManager !== null && callbackManager !== void 0 ? callbackManager : getCallbackManager();
        this.verbose = verbose !== null && verbose !== void 0 ? verbose : getVerbosity();
        this.cache = cache;
        this.concurrency = concurrency !== null && concurrency !== void 0 ? concurrency : Infinity;
        this.queue = new p_queue_1.default({ concurrency: this.concurrency });
    }
    /** @ignore */
    async _generateUncached(prompts, stop) {
        var _a, _b, _c, _d, _e, _f;
        (_b = (_a = this.callbackManager).handleStart) === null || _b === void 0 ? void 0 : _b.call(_a, { name: this.name }, prompts, this.verbose);
        let output;
        try {
            output = await this.queue.add(() => this._generate(prompts, stop), {
                throwOnTimeout: true,
            });
        }
        catch (err) {
            (_d = (_c = this.callbackManager).handleError) === null || _d === void 0 ? void 0 : _d.call(_c, `${err}`, this.verbose);
            throw err;
        }
        (_f = (_e = this.callbackManager).handleEnd) === null || _f === void 0 ? void 0 : _f.call(_e, output, this.verbose);
        return output;
    }
    /**
     * Run the LLM on the given propmts an input, handling caching.
     */
    async generate(prompts, stop) {
        var _a;
        if (!Array.isArray(prompts)) {
            throw new Error("Argument 'prompts' is expected to be a string[]");
        }
        if (this.cache === true && cache === null) {
            throw new Error("Requested cache, but no cache found");
        }
        if (cache === null || this.cache === false) {
            return this._generateUncached(prompts, stop);
        }
        const params = this.serialize();
        params.stop = stop;
        const llmStringKey = `${Object.entries(params).sort()}`;
        const missingPromptIndices = [];
        const generations = await Promise.all(prompts.map(async (prompt, index) => {
            const result = cache.lookup(await (0, cache_1.getKey)(prompt, llmStringKey));
            if (!result) {
                missingPromptIndices.push(index);
            }
            return result;
        }));
        let llmOutput = {};
        if (missingPromptIndices.length > 0) {
            const results = await this._generateUncached(missingPromptIndices.map((i) => prompts[i]), stop);
            await Promise.all(results.generations.map(async (generation, index) => {
                const promptIndex = missingPromptIndices[index];
                generations[promptIndex] = generation;
                const key = await (0, cache_1.getKey)(prompts[promptIndex], llmStringKey);
                cache.update(key, generation);
            }));
            llmOutput = (_a = results.llmOutput) !== null && _a !== void 0 ? _a : {};
        }
        return { generations, llmOutput };
    }
    /**
     * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.
     */
    async call(prompt, stop) {
        const { generations } = await this.generate([prompt], stop);
        return generations[0][0].text;
    }
    /**
     * Get the identifying parameters of the LLM.
     */
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    _identifyingParams() {
        return {};
    }
    /**
     * Return a json-like object representing this LLM.
     */
    serialize() {
        return {
            ...this._identifyingParams(),
            _type: this._llmType(),
        };
    }
    /**
     * Load an LLM from a json-like object describing it.
     */
    static async deserialize(data) {
        const { _type, ...rest } = data;
        const Cls = {
            openai: index_1.OpenAI,
        }[_type];
        if (Cls === undefined) {
            throw new Error(`Cannot load  LLM with type ${_type}`);
        }
        return new Cls(rest);
    }
    getNumTokens(text) {
        // TODOs copied from py implementation
        // TODO: this method may not be exact.
        // TODO: this method may differ based on model (eg codex).
        return (0, gpt_3_encoder_1.encode)(text).length;
    }
}
exports.BaseLLM = BaseLLM;
/**
 * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.
 *
 * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.
 *
 * @augments BaseLLM
 */
class LLM extends BaseLLM {
    async _generate(prompts, stop) {
        const generations = [];
        for (let i = 0; i < prompts.length; i += 1) {
            const text = await this.queue.add(() => this._call(prompts[i], stop), {
                throwOnTimeout: true,
            });
            generations.push([{ text }]);
        }
        return { generations };
    }
}
exports.LLM = LLM;
//# sourceMappingURL=base.js.map