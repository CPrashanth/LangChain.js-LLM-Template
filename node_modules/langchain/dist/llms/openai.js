"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.PromptLayerOpenAI = exports.OpenAI = void 0;
const node_fetch_1 = __importDefault(require("node-fetch"));
const eventsource_parser_1 = require("eventsource-parser");
const exponential_backoff_1 = require("exponential-backoff");
const util_1 = require("../util");
const _1 = require(".");
let Configuration = null;
let OpenAIApi = null;
try {
    // eslint-disable-next-line global-require,import/no-extraneous-dependencies
    ({ Configuration, OpenAIApi } = require("openai"));
}
catch (_a) {
    // ignore error
}
/**
 * Wrapper around OpenAI large language models.
 *
 * To use you should have the `openai` package installed, with the
 * `OPENAI_API_KEY` environment variable set.
 *
 * @remarks
 * Any parameters that are valid to be passed to {@link
 * https://platform.openai.com/docs/api-reference/completions/create |
 * `openai.createCompletion`} can be passed through {@link modelKwargs}, even
 * if not explicitly available on this class.
 *
 * @augments BaseLLM
 * @augments OpenAIInput
 */
class OpenAI extends _1.BaseLLM {
    constructor(fields, configuration) {
        var _a, _b, _c, _d, _e, _f, _g, _h, _j, _k, _l, _m, _o;
        super(fields === null || fields === void 0 ? void 0 : fields.callbackManager, fields === null || fields === void 0 ? void 0 : fields.verbose, fields === null || fields === void 0 ? void 0 : fields.concurrency, fields === null || fields === void 0 ? void 0 : fields.cache);
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0.7
        });
        Object.defineProperty(this, "maxTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 256
        });
        Object.defineProperty(this, "topP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "frequencyPenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0
        });
        Object.defineProperty(this, "presencePenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0
        });
        Object.defineProperty(this, "n", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "bestOf", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "logitBias", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "modelName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "text-davinci-003"
        });
        Object.defineProperty(this, "modelKwargs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "batchSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 20
        });
        Object.defineProperty(this, "maxRetries", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 6
        });
        Object.defineProperty(this, "stop", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "clientConfig", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        if (Configuration === null || OpenAIApi === null) {
            throw new Error("Please install openai as a dependency with, e.g. `npm i openai`");
        }
        this.modelName = (_a = fields === null || fields === void 0 ? void 0 : fields.modelName) !== null && _a !== void 0 ? _a : this.modelName;
        this.modelKwargs = (_b = fields === null || fields === void 0 ? void 0 : fields.modelKwargs) !== null && _b !== void 0 ? _b : {};
        this.batchSize = (_c = fields === null || fields === void 0 ? void 0 : fields.batchSize) !== null && _c !== void 0 ? _c : this.batchSize;
        this.maxRetries = (_d = fields === null || fields === void 0 ? void 0 : fields.maxRetries) !== null && _d !== void 0 ? _d : this.maxRetries;
        this.temperature = (_e = fields === null || fields === void 0 ? void 0 : fields.temperature) !== null && _e !== void 0 ? _e : this.temperature;
        this.maxTokens = (_f = fields === null || fields === void 0 ? void 0 : fields.maxTokens) !== null && _f !== void 0 ? _f : this.maxTokens;
        this.topP = (_g = fields === null || fields === void 0 ? void 0 : fields.topP) !== null && _g !== void 0 ? _g : this.topP;
        this.frequencyPenalty = (_h = fields === null || fields === void 0 ? void 0 : fields.frequencyPenalty) !== null && _h !== void 0 ? _h : this.frequencyPenalty;
        this.presencePenalty = (_j = fields === null || fields === void 0 ? void 0 : fields.presencePenalty) !== null && _j !== void 0 ? _j : this.presencePenalty;
        this.n = (_k = fields === null || fields === void 0 ? void 0 : fields.n) !== null && _k !== void 0 ? _k : this.n;
        this.bestOf = (_l = fields === null || fields === void 0 ? void 0 : fields.bestOf) !== null && _l !== void 0 ? _l : this.bestOf;
        this.logitBias = fields === null || fields === void 0 ? void 0 : fields.logitBias;
        this.stop = fields === null || fields === void 0 ? void 0 : fields.stop;
        this.streaming = (_m = fields === null || fields === void 0 ? void 0 : fields.streaming) !== null && _m !== void 0 ? _m : false;
        if (this.streaming && this.n > 1) {
            throw new Error("Cannot stream results when n > 1");
        }
        if (this.streaming && this.bestOf > 1) {
            throw new Error("Cannot stream results when bestOf > 1");
        }
        this.clientConfig = {
            apiKey: (_o = fields === null || fields === void 0 ? void 0 : fields.openAIApiKey) !== null && _o !== void 0 ? _o : process.env.OPENAI_API_KEY,
            ...configuration,
        };
        const clientConfig = new Configuration(this.clientConfig);
        this.client = new OpenAIApi(clientConfig);
    }
    /**
     * Get the parameters used to invoke the model
     */
    invocationParams() {
        return {
            model: this.modelName,
            temperature: this.temperature,
            max_tokens: this.maxTokens,
            top_p: this.topP,
            frequency_penalty: this.frequencyPenalty,
            presence_penalty: this.presencePenalty,
            n: this.n,
            best_of: this.bestOf,
            logit_bias: this.logitBias,
            stop: this.stop,
            stream: this.streaming,
            ...this.modelKwargs,
        };
    }
    _identifyingParams() {
        return {
            model_name: this.modelName,
            ...this.invocationParams(),
            ...this.clientConfig,
        };
    }
    /**
     * Get the identifying parameters for the model
     */
    identifyingParams() {
        return this._identifyingParams();
    }
    /**
     * Call out to OpenAI's endpoint with k unique prompts
     *
     * @param prompts - The prompts to pass into the model.
     * @param [stop] - Optional list of stop words to use when generating.
     *
     * @returns The full LLM output.
     *
     * @example
     * ```ts
     * import { OpenAI } from "langchain/llms";
     * const openai = new OpenAI();
     * const response = await openai.generate(["Tell me a joke."]);
     * ```
     */
    async _generate(prompts, stop) {
        var _a, _b, _c, _d;
        const subPrompts = (0, util_1.chunkArray)(prompts, this.batchSize);
        const choices = [];
        const tokenUsage = {};
        if (this.stop && stop) {
            throw new Error("Stop found in input and default params");
        }
        const params = this.invocationParams();
        params.stop = stop !== null && stop !== void 0 ? stop : params.stop;
        for (let i = 0; i < subPrompts.length; i += 1) {
            const { data } = await this.completionWithRetry({
                ...params,
                prompt: subPrompts[i],
            });
            if (params.stream) {
                const choice = await new Promise((resolve, reject) => {
                    const choice = {};
                    const parser = (0, eventsource_parser_1.createParser)((event) => {
                        var _a, _b, _c, _d, _e;
                        if (event.type === "event") {
                            if (event.data === "[DONE]") {
                                resolve(choice);
                            }
                            else {
                                const response = JSON.parse(event.data);
                                const part = response.choices[0];
                                if (part != null) {
                                    choice.text = ((_a = choice.text) !== null && _a !== void 0 ? _a : "") + ((_b = part.text) !== null && _b !== void 0 ? _b : "");
                                    choice.finish_reason = part.finish_reason;
                                    choice.logprobs = part.logprobs;
                                    (_d = (_c = this.callbackManager).handleNewToken) === null || _d === void 0 ? void 0 : _d.call(_c, (_e = part.text) !== null && _e !== void 0 ? _e : "", this.verbose);
                                }
                            }
                        }
                    });
                    // workaround for incorrect axios types
                    const stream = data;
                    stream.on("data", (data) => parser.feed(data.toString("utf-8")));
                    stream.on("error", (error) => reject(error));
                });
                choices.push(choice);
            }
            else {
                choices.push(...data.choices);
            }
            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = (_a = data.usage) !== null && _a !== void 0 ? _a : {};
            if (completionTokens) {
                tokenUsage.completionTokens =
                    ((_b = tokenUsage.completionTokens) !== null && _b !== void 0 ? _b : 0) + completionTokens;
            }
            if (promptTokens) {
                tokenUsage.promptTokens = ((_c = tokenUsage.promptTokens) !== null && _c !== void 0 ? _c : 0) + promptTokens;
            }
            if (totalTokens) {
                tokenUsage.totalTokens = ((_d = tokenUsage.totalTokens) !== null && _d !== void 0 ? _d : 0) + totalTokens;
            }
        }
        const generations = (0, util_1.chunkArray)(choices, this.n).map((promptChoices) => promptChoices.map((choice) => {
            var _a;
            return ({
                text: (_a = choice.text) !== null && _a !== void 0 ? _a : "",
                generationInfo: {
                    finishReason: choice.finish_reason,
                    logprobs: choice.logprobs,
                },
            });
        }));
        return {
            generations,
            llmOutput: { tokenUsage },
        };
    }
    /** @ignore */
    completionWithRetry(request) {
        const makeCompletionRequest = async () => this.client.createCompletion(request, request.stream ? { responseType: "stream" } : undefined);
        return (0, exponential_backoff_1.backOff)(makeCompletionRequest, {
            startingDelay: 4,
            maxDelay: 10,
            numOfAttempts: this.maxRetries,
            // TODO(sean) pass custom retry function to check error types.
        });
    }
    _llmType() {
        return "openai";
    }
}
exports.OpenAI = OpenAI;
/**
 * PromptLayer wrapper to OpenAI
 * @augments OpenAI
 */
class PromptLayerOpenAI extends OpenAI {
    constructor(fields) {
        var _a, _b;
        super(fields);
        Object.defineProperty(this, "promptLayerApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "plTags", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        this.plTags = (_a = fields === null || fields === void 0 ? void 0 : fields.plTags) !== null && _a !== void 0 ? _a : [];
        this.promptLayerApiKey =
            (_b = fields === null || fields === void 0 ? void 0 : fields.promptLayerApiKey) !== null && _b !== void 0 ? _b : process.env.PROMPTLAYER_API_KEY;
        if (!this.promptLayerApiKey) {
            throw new Error("Missing PromptLayer API key");
        }
    }
    async completionWithRetry(request) {
        var _a;
        if (request.stream) {
            return super.completionWithRetry(request);
        }
        const requestStartTime = Date.now();
        const response = await super.completionWithRetry(request);
        const requestEndTime = Date.now();
        // https://github.com/MagnivOrg/promptlayer-js-helper
        await (0, node_fetch_1.default)("https://api.promptlayer.com/track-request", {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                Accept: "application/json",
            },
            body: JSON.stringify({
                function_name: "openai.Completion.create",
                args: [],
                kwargs: { engine: request.model, prompt: request.prompt },
                tags: (_a = this.plTags) !== null && _a !== void 0 ? _a : [],
                request_response: response.data,
                request_start_time: Math.floor(requestStartTime / 1000),
                request_end_time: Math.floor(requestEndTime / 1000),
                api_key: process.env.PROMPTLAYER_API_KEY,
            }),
        });
        return response;
    }
}
exports.PromptLayerOpenAI = PromptLayerOpenAI;
//# sourceMappingURL=openai.js.map